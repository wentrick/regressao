---
title: "Análise de Regressão"
author: "Davi Wentrick Feijó"
date: "2023-04-17"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse,knitr,formatR)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 70), tidy = TRUE)
```

#### Exemplo 1 

A taxa de metabolismo é importante em estudos sobre aumento de peso, dieta e exercício. Em um estudo com 19 indivíduos selecionados aleatoriamente entre os submetidos a um estudo de dieta, foram coletados dados sobre a massa do corpo sem gordura e a taxa metabólica em repouso. A massa do corpo sem gordura é o peso da pessoa, eliminada toda a gordura, e é dada em quilogramas. A taxa de metabolismo é medida em calorias queimadas a cada 24 horas e os pesquisadores acham que a massa do corpo sem gordura tem grande influência sobre ela.

```{r include=FALSE}
#exercicios de regressao onde x é a massa sem gordura e y é a taxa do metabolismo

massa = c(62.0,62.9,36.1,54.6,48.5,42.0,47.4,50.6,42.0,48.7,40.3,33.1,51.9,42.4,34.5,51.1,41.2,51.9,46.9)
taxa = c(1792,1666,995,1425,1396,1418,1362,1502,1256,1614,1189,913,1460,1124,1052,1347,1204,1867,1439)

n = length(massa)

dados = data.frame(massa,taxa)
```

```{r echo=FALSE}
kable(dados)
```

\newpage

##### Calcule o estimadores abaixo manualmente (com arredondamentos):


$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n}X_i Y_i-n\bar{X}\bar{Y}}{\sum_{i=1}^{n}X_i^2 - n\bar{X}^2}
$$
$$
\hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{X}
$$
Os valores encontrados em sala foram os seguintes:


$$
\hat{\beta_0} = 117,44 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \hat{\beta_1} = 26,79
$$



Com os coeficientes obtidos a nossa equacao ficou assim:

$$
\hat{Y} = 117,44 + 26,79x_{}
$$
Aplicando a equacao encontrada nos dados podemos obter o valor estimado (sabendo que x = massa)
```{r include=FALSE}
#vamos calcular o beta 0  e beta 1 manualmente com arredondamentos
#valores encontrados em sala

dados = dados %>%
  mutate(estimado = 117.44 +26.79*massa,
         residuo = taxa - estimado,
         taxa_quadrado = taxa**2,
         massa_quadrado = massa**2,
         massa_x_taxa = massa*taxa)

erro = sum(dados$residuo)
```

```{r echo=FALSE}
kable(dados[1:3])
```

\newpage

Para verificar se esse valor estimado está condizente podemos calcular os residuos e soma-los, devemos encontrar que a soma dos reisudos é 0. Dado que a forma de se calcular os residuos segue essa funcao:

$$
e_i = y_i - \hat{y_i}
$$


onde e_i é o resíduo para a i-ésima observação, $y_i$ é o valor observado da variável dependente para a i-ésima observação e $\hat{Y_i}$ é o valor previsto pela reta de regressão para a i-ésima observação.

```{r echo=FALSE}
kable(dados[1:4])
```



```{r echo=TRUE}
erro
```
$$
\sum_{i = 1}^{n} y_i - \hat{y_i} = -2,559
$$
\newpage


##### Calculando os parametros $\beta_1$ e $\beta_0$ computacionalmente:

Com o resultado anterior podemos perceber que devido aos arredondamentos feitos em sala, a soma dos residuos nao deu zero! Agora vamos calcular $\beta_1$ e $\beta_0$ por meio do R. Fazendo os calculos necessarios temos que:
```{r include=FALSE}
x_barra = mean(dados$massa) #media amostral de x 
y_barra = mean(dados$taxa) #media amostra de y
x = sum(dados$massa) #somatorio de xi
y = sum(dados$taxa) #somatorio de yi
x_quadrado = sum(dados$massa_quadrado) #somatorio de xi ao quadrado
y_quadrado = sum(dados$taxa_quadrado) #somatorio de yi ao quadrado
xy = sum(dados$massa_x_taxa) #somatorio de x vezes y

beta_1 = (xy - n*x_barra*y_barra)/(x_quadrado-n*(x_barra)**2) #calculo do estimador de beta 1
beta_0 = y_barra - beta_1*x_barra #calculo do estimador de beta 0

dados = dados %>%
  mutate(estimado = 117.44 +26.79*massa,
         estimado_melhor = beta_0+beta_1*massa,
         residuo = taxa - estimado,
         residuo_melhor = taxa - estimado_melhor,
         taxa_quadrado = taxa**2,
         massa_quadrado = massa**2,
         massa_x_taxa = massa*taxa)
erro = sum(dados$residuo_melhor)
```


```{r include=FALSE}
kable(dados%>%
        select(1,2,5,6,7))
```


$$
\bar{Y} = 1.369,526 \ \ \ \ \bar{X} = 46,74211 
$$
$$
\sum_{n=1}^{n}X_i = 888,1 \ \ \ \ \sum_{n=1}^{n}Y_i = 26.021
$$
$$
\sum_{n=1}^{n}X_i^2 = 42747,03 \ \ \ \ \sum_{n=1}^{n}Y_i^2 = 36.829.995 \ \ \ \ \sum_{n=1}^{n}X_iY_i = 1.249.481
$$

Com esses resultados podemos aplicar na formual dos estimadores de $\beta_1$ e $\beta_0$

$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n}X_i Y_i-n\bar{X}\bar{Y}}{\sum_{i=1}^{n}X_i^2 - n\bar{X}^2} \ \ \ \ \hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{X}
$$
e assim encontramos:
 
$$
\hat{\beta_1} = \frac{1.249.481-19 \cdot 46,74211 \cdot 1.369,526}{42747,03 - 19 \cdot 46,74211^2} = 26,87857 \ \ \ \ \ \ \ \hat{\beta_0} = 1.369,526 - 26.87857 \cdot 46,74211 = 113,1654
$$

Com os novos coeficientes obtidos de $\beta_1$ e $\beta_0$ a nossa equacao ficou assim:

$$
\hat{Y} = 113,1654 + 26,87857x_{}
$$
Agora podemos recalcular o valor estimado e depois fazer o calculo dos residuos.
```{r echo=FALSE}
kable(dados%>%
        select(Massa = massa,Taxa = taxa, `Novo estimado` = estimado_melhor,`Novo residuo` = residuo_melhor))
```

\newpage

realizando o calculo da soma de reisiduos novamente podemos verificar que dessa vez devido a maior precisao atingimos o 0

```{r echo=TRUE}
erro
round(erro,4)
```

Podemos rodar o modelo completo no R para ter nocao dos resultado!

```{r}
lm(dados$taxa ~ dados$massa) %>% summary()
```



\newpage


##### Calcule o estimador da variancia dos erros :



Podemos estimar o $\sigma^2$ da variancia dos erros por meio da seguinte formula:


$$
\hat\sigma^2 = \frac{\sum_{i=1}^{n}e_i^2}{n-2}
$$
```{r include=FALSE}
erro_quadrado = sum(dados$residuo_melhor**2)

sigma_quadrado = erro_quadrado/(n - 2)

sigma = sqrt(sigma_quadrado)

erro_padrao = sigma/sqrt(n)

coeficiente_variacao = sigma/y_barra #desvio padrao divido pela media
```


Realizando os calculos no R temos:


$$
\hat\sigma^2 = \frac{\sum_{i=1}^{n}e_i^2}{n-2} = \frac{301.051,1}{19-2} = \frac{301.051,1}{17} = 17.708,89 \ \ \ \ \ \hat\sigma = \sqrt{\hat\sigma^2} = 133,0747
$$
Podemos calcular o erro padrao por meio da seguinte formula:

$$
SE = \frac{s}{\sqrt{n}}
$$
onde "s" representa o desvio padrão da amostra e "n" é o tamanho da amostra. O símbolo "SE" é usado para indicar o erro padrão.

Realizando os calculos obtemos o seguinte valor:


$$
SE = \frac{s}{\sqrt{n}}= \frac{133,0747}{\sqrt{19}} = 30,52944
$$

para ter uma nocao da variabilidade pode calcular o coeficiente de variação (CV) que é uma medida relativa de variabilidade que é frequentemente usada para comparar a variabilidade relativa entre diferentes conjuntos de dados. É definido como o desvio padrão dividido pela média, expresso como uma porcentagem.

$$
CV = \frac{s}{\overline{x}} \times 100\%
$$

onde "s" representa o desvio padrão da amostra e $\overline{x}$ representa a média da amostra. O símbolo "%" indica que o resultado é expresso como uma porcentagem.

Utilizando o R para o calculo obtemos o seguinte resultado:

$$
CV = \frac{s}{\overline{x}} \times 100 = \frac{133,0747}{1.369,526} \times 100 = 9,716845\%
$$
\newpage

##### Calculando o estimador da variancia de $\beta_1$ e $\beta_0$:


```{r include=FALSE}
s2_beta_1 = sigma_quadrado/(x_quadrado - n* x_barra^2)

s_beta_1 = sqrt(s2_beta_1)

coeficiente_variacao_beta_1 = s_beta_1/beta_1


s2_beta_0 =  sigma_quadrado*(x_quadrado/(n*(x_quadrado - n*x_barra**2)))

s_beta_0 = sqrt(s2_beta_0)

coeficiente_variacao_beta_0 = s_beta_0/beta_0 #variancia maior pois incorpara a variancia de beta 1 (no caso ele repsresenta someente o intercepto ent nao faz mt diferenca)

```

As formulas para o calculo sao as seguintes:

$$
s^2(\beta_0) = V(\beta_0) = \hat\sigma^2 [\frac{\sum_{i=1}^{n}X_i^2}{n(\sum_{i=1}^{n}X_i^2 - n\bar{X}^2)}]
$$

$$
s^2(\beta_1) = V(\beta_1) =  \frac{\hat\sigma^2}{(\sum_{i=1}^{n}X_i^2 - n\bar{X}^2)}
$$
Como já temos esses valores calculados é só substituir na formula!


$$
s^2(\beta_1)  =  \frac{17.708,89}{(42.747,03 - 19 \cdot 46,74211^2)} = 14,33493 \ \ \ \ \ s(\beta_1)  = \sqrt{s^2(\beta_1)} =\sqrt{14,33493} =  3,78615 
$$

$$
s^2(\beta_0) = 17.708,89 [\frac{42.747,03}{19 \cdot (42.747,03 - 19 \cdot 46,74211^2)}] = 32.251,35 \ \ \ \ \ s(\beta_0)  = \sqrt{s^2(\beta_0)} =\sqrt{32.251,35} =  179,5866
$$

Vamos calcular os Coeficientes de variacao dos valores estimados da variancia do erro.

$$
CV\beta_1 = \frac{s(\beta_1)}{\beta_1} \times 100 = \frac{3,78615}{26,87857} \times 100 = 14,08613\%
$$
$$
CV\beta_0 = \frac{s(\beta_0)}{\beta_0} \times 100 = \frac{179,5866}{113,1654} \times 100 = 158,6939\%
$$

Vale  notar que a variancia de $\beta_0$ é maior pois incorpara a variancia de $\beta_1$ (no caso ele representa somente o intercepto entao nao faz muita diferenca nesse exemplo em especifico)

\newpage

##### Vamos calcular um intervalo de confianca para $\beta_0$ e $\beta_1$:

Sabemos que a estatistica : 



$$
\frac{\hat\beta_1 - \beta_1}{S(\hat\beta_1)} \sim t(n-2)
$$

$$
IC = P(-t_{1-\alpha/2} < \frac{\hat\beta_1 - \beta_1}{S(\beta_1)} < t_{1-\alpha/2})
$$
Isolando o $\beta_1$ obtemos:
$$
IC = P(\hat\beta_1 - t_{1-\alpha/2}\cdot S(\hat\beta_1) <  \beta_1 < \hat\beta_1 + t_{1-\alpha/2} \cdot S(\hat\beta_1)) = \hat\beta_1 \pm t_{1-\alpha/2}\cdot S(\hat\beta_1)
$$

Sabendo que $S(\hat\beta_1)$ é igual a $\frac{\hat\sigma}{\sqrt{\sum_{i=1}^{n}X_i^2 - n\bar{X}^2}}$

Chegamos na formula final para o intervalo de confianca:

$$
IC = \hat\beta_1 \pm t_{1-\alpha/2}\cdot \frac{\hat\sigma}{\sqrt{\sum_{i=1}^{n}X_i^2 - n\bar{X}^2}})
$$
Seguindo a mesma deducao e sabendo que $S(\hat\beta_0) =  \frac{\hat\sigma \cdot \sum_{i=1}^{n}X_i^2}{\sqrt{n(\sum_{i=1}^{n}X_i^2 - n\bar{X}^2)}}$ obtemos:


$$
IC = \hat\beta_0 \pm t_{1-\alpha/2}\cdot \frac{\hat\sigma \cdot \sum_{i=1}^{n}X_i^2}{\sqrt{n(\sum_{i=1}^{n}X_i^2 - n\bar{X}^2)}})
$$

Fazendo os calculos no R obtemos os seguintes intervaloes para $\beta_0$ e $\beta_1$:

```{r}
#estimando intervalo de confianca para beta 1 com 95% de confianca 
alfa = 0.05

ic_beta_1 = c(beta_1-qt(alfa/2, n-2,lower.tail = FALSE)*s_beta_1,beta_1+qt(alfa/2, n-2,lower.tail = FALSE)*s_beta_1)

#estimando intervalo de confianca para beta 0 com 95% de confianca 
alfa = 0.05

ic_beta_0 = c(beta_0-qt(alfa/2, n-2,lower.tail = FALSE)*s_beta_0,beta_0+qt((alfa/2), n-2,lower.tail = FALSE)*s_beta_0)
```


```{r}
ic_beta_1
```

```{r}
ic_beta_0
```

Podemos verificar isso por meio da funcao lm() que roda a regressao por completa!


```{r}
fit <- lm(dados$taxa ~ dados$massa)

# Cálculo do intervalo de confiança para beta1 com nível de significância de 0.05
alpha <- 0.05
se_b1 <- summary(fit)$coefficients[2, 2]
beta1 <- summary(fit)$coefficients[2, 1]
t_crit <- qt(1 - alpha/2, df = fit$df.residual)
lower_b1 <- beta1 - t_crit * se_b1
upper_b1 <- beta1 + t_crit * se_b1
cat("Intervalo de confiança para beta1: [", lower_b1, ", ", upper_b1, "]\n")
# Cálculo do intervalo de confiança para beta0 com nível de significância de 0.05
alpha <- 0.05
se_b0 <- summary(fit)$coefficients[1, 2]
beta0 <- summary(fit)$coefficients[1, 1]
t_crit <- qt(1 - alpha/2, df = fit$df.residual)
lower_b0 <- beta0 - t_crit * se_b0
upper_b0 <- beta0 + t_crit * se_b0
cat("Intervalo de confiança para beta0: [", lower_b0, ", ", upper_b0, "]\n")
```
\newpage

##### Vamos formular os teste de hipoteses sobre $\beta_0$ e $\beta_1$

$$
H_0: \beta_1 = 0 \ \ \ \sim \ \ \ H_0: \text{Ausencia de regressao}
$$

$$
H_1: \beta_1 \neq 0 \ \ \ \sim \ \ \ H_1: \text{Existe de regressao}
$$


A estatistica do teste é dado por:

$$
T = \frac{\hat\beta_1}{s(\hat\beta_1)} : \text{distribuição de Student com (n-2) g.l}
$$

```{r}
#Ho: beta_1 = 0
#H0: beta_1 > 0 (ou != 0)
alfa = 0.05
teste_stat = beta_1/s_beta_1

rc = qt(alfa,n-2) #regiao critica

pvalor = pt(teste_stat,n-2, lower.tail = FALSE) #pvalor

result = pvalor < alfa #pvalor menor que o alfa de 5% logo rejeitamos H0
cat("O P-Valor observado foi:",pvalor, "O valor observado pertence a regiao crítica de",alfa,"?:",result)

```

Logo existe uma regressao ou seja $\beta_1$ contribui para a relacao da massa sem gordura e o metabolismo

O teste de hipotese sobre $\beta_0$ segue os mesmos parametros do teste anterior:

$$
H_0: \beta_0 = \beta 
$$

$$
H_1: \beta_0 \neq \beta 
$$
A estatistica do teste é dado por:

$$
T = \frac{\hat\beta_0}{s(\hat\beta_0)} : \text{distribuição de Student com (n-2) g.l}
$$
Esse teste busca testar um valor de $\beta_0$ contra algum outro valor de interesse

\newpage

##### Intervalo de confianca sobre $\sigma^2$


Temos que:

$$
\frac{(n-2)\hat\sigma^2}{\sigma^2}: \text{distribuição de Qui-Quadrado com (n-2) g.l}
$$

Fixada uma probabilidade de de $1-\alpha$ podemos determinar $\chi_{\alpha/2}$ e $\chi_{1-\alpha/2}$ para chegar na seguinte formula:

$$
P(\chi_{\alpha/2} < \frac{(n-2)\hat\sigma^2}{\sigma^2} < \chi_{1-\alpha/2}) = 1-\alpha
$$

$$
P(\frac{(n-2)\hat\sigma^2}{\chi_{\alpha/2}} < \sigma^2 < \frac{(n-2)\hat\sigma^2}{\chi_{1-\alpha/2}}) = 1-\alpha
$$

Obtemos o seguinte intervalo:

$$
\sigma^2 \in (\frac{(n-2)\hat\sigma^2}{\chi_{\alpha/2}} , \frac{(n-2)\hat\sigma^2}{\chi_{1-\alpha/2}}) 
$$

Relembrando que:

$$
(n-2)\hat\sigma^2 = \sum_{i=1}^{n} e_{i}^2
$$


Podemos chegar nesse novo intervalo:

$$
\sigma^2 \in (\frac{\sum_{i=1}^{n} e_{i}^2}{\chi_{\alpha/2}} , \frac{\sum_{i=1}^{n} e_{i}^2}{\chi_{1-\alpha/2}}) 
$$

```{r}
gl = n-2
alfa = 0.05
#intervalo usando a formula do primeiro intervalo encontrado
ic_sigma_quadrado = c(((n-2)*sigma_quadrado/qchisq(alfa/2,gl,lower.tail = FALSE)),((n-2)*sigma_quadrado/qchisq(1-alfa/2,gl,lower.tail = FALSE)))
cat("Intervalo de confiança para sigma quadrado: [", ic_sigma_quadrado[1], ", ", ic_sigma_quadrado[2], "]\n")
```


```{r}

#intervalo usando a soma dos residuos ao quadrado
ic_sigma_quadrado = c(erro_quadrado/qchisq(alfa/2,gl,lower.tail = FALSE),erro_quadrado/qchisq(1-(alfa/2),gl,lower.tail = FALSE))

cat("Intervalo de confiança para sigma quadrado: [", ic_sigma_quadrado[1], ", ", ic_sigma_quadrado[2], "]\n")
```




